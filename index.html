<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>David Klindt</title>
  
  <meta name="author" content="David Klindt">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>David Klindt</name>
              </p>
              <p align="justify">
                I am a postdoctoral researcher in the group of <a href="https://www.ntnu.edu/employees/benjamin.dunn">Benjamin Dunn</a>, where I work on machine learning, computational neuroscience and computer vision.
              </p>
              <p align="justify">
                Prior to that, I did my PhD at the <a href="https://uni-tuebingen.de/">University of Tübingen</a>, where I was advised by <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a> and <a href="http://www.eye-tuebingen.de/euler/">Thomas Euler</a> and worked on biological and computer vision;
		        I did my masters in <a href="https://www.ucl.ac.uk/ion/brain-and-mind-sciences-msc">Brain and Mind Sciences</a> at <a href="https://www.ucl.ac.uk">University College London</a>, <a href="https://www.ens.psl.eu/en">École Normale Supérieure</a> and <a href="http://www.upmc.fr">Pierre and Marie Curie University</a> in Paris;
		        And I did my bachelors at the <a href="https://www.uni-magdeburg.de/">Otto von Guericke University</a>.
              </p>
              <p align="justify">
                If you are a student looking for internship, bachelor or master’s thesis opportunities, feel free to reach out.
              </p>
              <p style="text-align:center">
                <a href="mailto:klindt.david@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.de/citations?user=EpT-nUAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/david-klindt">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/david-klindt-b03833189/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://twitter.com/klindt_david">Twitter</a> &nbsp/&nbsp
                <a href="https://www.youtube.com/channel/UC-hXTCXQqveQlWyJucdltAA?view_as=subscriber">YouTube</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/david.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/david.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p align="justify">
                I'm interested in machine learning, computer vision and
                computational neuroscience. Recently, I have been working on
                generative models, disentanglement (nonlinear ICA), domain
                adaptation, robustness, topological data analysis and
                different topics in computational neuroscience.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/slowvae.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.10930">
                <papertitle>Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding</papertitle>
              </a>
              <br>
              <strong>David Klindt*</strong>,
              <a href="https://scholar.google.de/citations?user=I4mXKS4AAAAJ&amp;hl=en">Lukas Schott*</a>,
              <a href="https://scholar.google.com/citations?user=AlGCn8wAAAAJ&hl=en">Yash Sharma*</a>,
              <a href="https://scholar.google.com/citations?user=YGEMpYUAAAAJ&amp;hl=en">Ivan Ustyuzhaninov</a>,
              <a href="https://scholar.google.com/citations?user=v-JL-hsAAAAJ&amp;hl=en">Wieland Brendel</a>,
              <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a>,
              <a href="https://scholar.google.com/citations?user=HYQYE3gAAAAJ&amp;hl=en">Dylan Paiton</a> (*equal contribution)
              <br>
              <em>ICLR</em>, 2021 &nbsp; (<font color="red"><strong>Oral,
            </strong></font> top 0.1% of submissions)
              <br>
              <a href="https://openreview.net/forum?id=EbIDjBynYJ8">paper</a> /
              <a href="https://github.com/bethgelab/slow_disentanglement">code</a> /
              <a href="https://www.youtube.com/watch?v=M5ezZ5TFPcc&t=719s">talk</a>
              <p align="justify">
                We show that accounting for the temporally sparse nature of
                natural transitions leads to a proof of identifiability and
                reliable learning of disentangled representations on several
                established benchmark datasets, as well as contributed
                datasets with natural dynamics.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/yong.jpg" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://t.co/HMZJZuctcg?amp=1">
                <papertitle>Natural environment statistics in the upper and lower visual field are reflected in mouse retinal specializations</papertitle>
              </a>
              <br>
              Yongrong Qiu,
              Zhijian Zhao,
              <strong>David Klindt</strong>,
              Magdalena Kautzky,
              Klaudia P. Szatko,
              <a href="https://scholar.google.de/citations?user=SJlRyW8AAAAJ&hl=en&oi=sra">Frank Schaeffel</a>,
              Katharina Rifai,
              <a href="https://scholar.google.de/citations?user=zaBjCS8AAAAJ&hl=en&oi=ao">Katrin Franke</a>,
              Laura Busse,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a>
              <br>
              <em>Current Biology</em>, 2021
              <br>
              <a href="https://doi.org/10.1016/j.cub.2021.05.017">doi</a>
              <p align="justify">
                We record natural visual scenes of mouse habitats and show
                that these have different UV and green contrast levels. This
                is reflected in mice retinal circuits and facilitates predator
                detection.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cosyne21.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.cosyne.org/cosyne21/Cosyne2021_program_book.pdf">
                <papertitle>Modelling functional wiring and processing from retinal bipolar to ganglion cells</papertitle>
              </a>
              <br>
              <strong>David Klindt*</strong>,
              <a href="https://scholar.google.de/citations?user=HpLWKHEAAAAJ&hl=en&oi=ao">Cornelius Schröder*</a>,
              <a href="https://scholar.google.de/citations?user=_cXMEYEAAAAJ&hl=en&oi=ao">Anna Vlasits</a>,
              <a href="https://scholar.google.de/citations?user=zaBjCS8AAAAJ&hl=en&oi=ao">Katrin Franke</a>,
              <a href="https://scholar.google.de/citations?user=lPQLk3QAAAAJ&hl=en&oi=ao">Philipp Berens</a>,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a> (*equal contribution)
              (*equal contribution)
              <br>
              <em>Cosyne</em>, 2021
              <br>
              <a href="images/3-117_cosyne_poster_2021.png">poster</a> /
              <a href="https://www.youtube.com/watch?v=Qy_4xHSctco&t=16s">presentation
              </a>
              <p align="justify">
                We extend our model of the inner retina (see below) to
                predict the activity of ganglion cells, the next processing
                layer in the retina. The model matches known connectivity and
                stratification profiles.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/corni.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2020/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html">
                <papertitle>System Identification with Biophysical Constraints: A Circuit Model of the Inner Retina</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.de/citations?user=HpLWKHEAAAAJ&hl=en&oi=ao">Cornelius Schröder*</a>,
              <strong>David Klindt*</strong>,
              Sarah Strauss,
              <a href="https://scholar.google.de/citations?user=zaBjCS8AAAAJ&hl=en&oi=ao">Katrin Franke</a>,
              <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a>,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a>
              (*equal contribution)
              <br>
              <em>NeurIPS</em>, 2020 &nbsp; (<font
                    color="red"><strong>Spotlight</strong></font>)
              <br>
              <a href="https://www.biorxiv.org/content/biorxiv/early/2020/10/21/2020.06.16.154203.full.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=8TEOzD7x41E">talk</a>
              <p align="justify">
                We construct a mechanistically detailed but fully
                differentiable model of the inner retina. This performs well
                against black box deep learning models, reproduces essential
                retinal circuit functions and drug effects and makes new cell
                type specific experimental predictions.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/smt2.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://meetings.cshl.edu/abstracts.aspx?meet=NAISYS&year=20">
                <papertitle>Unsupervised Learning of Image Manifolds with Mutual Information</papertitle>
              </a>
              <br>
              <strong>David Klindt*</strong>,
              <a href="https://scholar.google.de/citations?user=uKDe38UAAAAJ&hl=en&oi=ao">Johannes Ballé*</a>,
              <a href="https://scholar.google.de/citations?user=x5bC-UwAAAAJ&hl=en&oi=sra">Jonathon Shlens</a>,
              <a href="https://scholar.google.de/citations?user=MplR7_cAAAAJ&hl=en&oi=ao">Eero Simoncelli</a>
              (*equal contribution)
              <br>
              <em>NAISys</em>, 2020
              <br>
              <a href="data/NAISYS_poster.pdf">poster</a>
              <p align="justify">
                We present a neural network layer and an unsupervised
                objective function that learns sparse features
                from natural images and projects them into a low dimensional
                space preserving their topology. Stacking such unsupervised
                layers yields a model that is not fooled by shortcuts on a
                benchmark dataset.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/zhao.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41598-020-60214-z">
                <papertitle>The temporal structure of the inner retina at a single glance</papertitle>
              </a>
              <br>
              Zhijian Zhao*,
              <strong>David Klindt*</strong>,
              André Maia Chagas,
              Klaudia P. Szatko,
              Luke Rogerson,
              Dario A. Protti,
              Christian Behrens,
              Deniz Dalkara,
              Timm Schubert,
              <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a>,
              <a href="https://scholar.google.de/citations?user=zaBjCS8AAAAJ&hl=en&oi=ao">Katrin Franke</a>,
              <a href="https://scholar.google.de/citations?user=lPQLk3QAAAAJ&hl=en&oi=ao">Philipp Berens</a>,
              <a href="https://scholar.google.de/citations?user=VgYU_m8AAAAJ&hl=en&oi=ao">Alexander S. Ecker</a>,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a> (*equal contribution)
              <br>
              <em>Nature Scientific Reports</em>, 2020
              <br>
              <a href="https://doi.org/10.1038/s41598-020-60214-z">doi</a>
              <p align="justify">
                With a novel electrically tunable lens, we record the entire
                inner retina which shows strong inter-experimental
                variability. We construct models adjusting for the response
                speed due to temperature fluctuations across experiments.
                These recover most of the response variability and, for the
                first time, allow a precise characterization of the kinetic
                response gradient across the inner retina.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cosyne19.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="http://cosyne.org/cosyne19/Cosyne2019_program_book.pdf">
                <papertitle>Adjusting For Batch Effects In Two Photon Imaging Recordings Of The Retinal Inner Plexiform Layer</papertitle>
              </a>
              <br>
              <strong>David Klindt*</strong>,
              Luke E. Rogerson*,
              Zhijian Zhao,
              Klaudia Szatko,
              <a href="https://scholar.google.de/citations?user=zaBjCS8AAAAJ&hl=en&oi=ao">Katrin Franke</a>,
              <a href="https://scholar.google.de/citations?user=BUQbD5kAAAAJ&hl=en&oi=sra">Dmitry Kobak</a>,
              <a href="https://scholar.google.de/citations?user=VgYU_m8AAAAJ&hl=en&oi=ao">Alexander S. Ecker</a>,
              <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a>,
              <a href="https://scholar.google.de/citations?user=lPQLk3QAAAAJ&hl=en&oi=ao">Philipp Berens</a>,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a> (*equal contribution)
              <br>
              <em>Cosyne</em>, 2019
              <br>
              <a href="data/cosyne19_poster.pdf">poster</a>
              </a>
              <p align="justify">
                We demonstrate strong inter-experimental variability for
                recordings of retinal Bipolar cells and propose a simple
                linear and a parametric method that adjust for these effects.
              </p>
            </td>
          <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/neurips17.png" width="100%">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2017/hash/8c249675aea6c3cbd91661bbae767ff1-Abstract.html">
                <papertitle>Neural system identification for large
                  populations – separating “what” and “where”</papertitle>
              </a>
              <br>
              <strong>David Klindt*</strong>,
              <a href="https://scholar.google.de/citations?user=VgYU_m8AAAAJ&hl=en&oi=ao">Alexander S. Ecker</a>,
              <a href="https://scholar.google.de/citations?user=XeqGapgAAAAJ&hl=en&oi=ao">Thomas Euler</a>,
              <a href="http://bethgelab.org/people/matthias/">Matthias Bethge</a> (*equal contribution)
              <br>
              <em>NeurIPS</em>, 2017
              <br>
              <a href="https://papers.nips.cc/paper/2017/file/8c249675aea6c3cbd91661bbae767ff1-Paper.pdf">paper</a> /
              <a href="https://github.com/david-klindt/NIPS2017">code</a>
              <p align="justify">
                We build a model for neural system identification with
                Convolutional Neural Networks, that outperforms the previous
                state of the art in predictive performance, but also allows
                the clustering of neurons into distinct functional cell types.
              </p>
            </td>
          <tr>


        </tbody></table>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
              Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for this iconic website template.  
	      </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
